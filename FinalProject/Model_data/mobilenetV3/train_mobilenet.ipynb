{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d4fa8a-9287-42a6-8a4b-5b53d8eb5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 資料增強與標準化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36508726-7055-45db-9766-358e0cede03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加載數據\n",
    "train_dataset = datasets.ImageFolder(root='../dataloader_c23/train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='../dataloader_c23/validation', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='../dataloader_c23/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb564426-f133-4b46-b7ae-c6fb8a4f678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device.\n"
     ]
    }
   ],
   "source": [
    "from mobilenetv3 import mobilenetv3_large\n",
    "# 設定設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "# 加載預訓練的 MobileNetV3 大模型\n",
    "model = mobilenetv3_large(num_classes=2)\n",
    "checkpoint = torch.load('mobilenetv3-large-1cd25616.pth', map_location=device)\n",
    "\n",
    "# 刪除分類層權重\n",
    "pre_dict = {k: v for k, v in checkpoint.items() if k in model.state_dict() and model.state_dict()[k].numel() == v.numel()}\n",
    "missing_keys, unexpected_keys = model.load_state_dict(pre_dict, strict=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a0774b-2681-4bb7-acf4-517b0a782c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練函數\n",
    "def train(epoch, epochs, model, train_loader, optimizer, loss_function, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "    \n",
    "    for step, data in enumerate(train_bar):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)  # 確保數據在正確的設備上\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)  # 模型輸出\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 計算準確率\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, epochs, loss)\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return running_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123eb835-880c-4703-8a14-2ba254ebaa17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 驗證函數\n",
    "def validate(epoch, epochs, model, validate_loader, loss_function, device):\n",
    "    model.eval()\n",
    "    acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_num = len(validate_loader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(validate_loader, file=sys.stdout)\n",
    "        for val_data in val_bar:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            outputs = model(val_images)\n",
    "            loss = loss_function(outputs, val_labels)\n",
    "            val_loss += loss.item() * val_images.size(0)\n",
    "\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            acc += torch.eq(predict_y, val_labels).sum().item()\n",
    "\n",
    "            val_bar.desc = \"valid epoch[{}/{}]\".format(epoch + 1, epochs)\n",
    "    \n",
    "    val_loss /= val_num\n",
    "    val_accurate = acc / val_num\n",
    "    return val_loss, val_accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ba9937-b282-4700-a9d6-a8b3f6c8ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/25] loss:0.109: 100%|██████████| 282/282 [01:35<00:00,  2.96it/s]\n",
      "valid epoch[1/25]: 100%|██████████| 55/55 [00:22<00:00,  2.40it/s]\n",
      "[epoch 1] train_loss: 0.233  train_accuracy: 89.940\n",
      "[epoch 1] val_loss: 0.347  val_accuracy: 0.877\n",
      "Training_Time: 118.26 seconds\n",
      "train epoch[2/25] loss:0.108: 100%|██████████| 282/282 [01:33<00:00,  3.03it/s]\n",
      "valid epoch[2/25]: 100%|██████████| 55/55 [00:21<00:00,  2.53it/s]\n",
      "[epoch 2] train_loss: 0.101  train_accuracy: 96.060\n",
      "[epoch 2] val_loss: 0.380  val_accuracy: 0.892\n",
      "Training_Time: 114.83 seconds\n",
      "train epoch[3/25] loss:0.088: 100%|██████████| 282/282 [01:32<00:00,  3.03it/s]\n",
      "valid epoch[3/25]: 100%|██████████| 55/55 [00:21<00:00,  2.56it/s]\n",
      "[epoch 3] train_loss: 0.071  train_accuracy: 97.231\n",
      "[epoch 3] val_loss: 0.311  val_accuracy: 0.885\n",
      "Training_Time: 114.51 seconds\n",
      "train epoch[4/25] loss:0.054: 100%|██████████| 282/282 [01:33<00:00,  3.00it/s]\n",
      "valid epoch[4/25]: 100%|██████████| 55/55 [00:21<00:00,  2.60it/s]\n",
      "[epoch 4] train_loss: 0.058  train_accuracy: 97.814\n",
      "[epoch 4] val_loss: 0.401  val_accuracy: 0.890\n",
      "Training_Time: 115.12 seconds\n",
      "train epoch[5/25] loss:0.028: 100%|██████████| 282/282 [01:32<00:00,  3.05it/s]\n",
      "valid epoch[5/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 5] train_loss: 0.044  train_accuracy: 98.299\n",
      "[epoch 5] val_loss: 0.398  val_accuracy: 0.888\n",
      "Training_Time: 114.09 seconds\n",
      "train epoch[6/25] loss:0.069: 100%|██████████| 282/282 [01:33<00:00,  3.03it/s]\n",
      "valid epoch[6/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 6] train_loss: 0.041  train_accuracy: 98.485\n",
      "[epoch 6] val_loss: 0.431  val_accuracy: 0.893\n",
      "Training_Time: 114.67 seconds\n",
      "train epoch[7/25] loss:0.026: 100%|██████████| 282/282 [01:32<00:00,  3.03it/s]\n",
      "valid epoch[7/25]: 100%|██████████| 55/55 [00:21<00:00,  2.61it/s]\n",
      "[epoch 7] train_loss: 0.036  train_accuracy: 98.661\n",
      "[epoch 7] val_loss: 0.442  val_accuracy: 0.889\n",
      "Training_Time: 114.05 seconds\n",
      "train epoch[8/25] loss:0.148: 100%|██████████| 282/282 [01:32<00:00,  3.05it/s]\n",
      "valid epoch[8/25]: 100%|██████████| 55/55 [00:21<00:00,  2.53it/s]\n",
      "[epoch 8] train_loss: 0.030  train_accuracy: 98.925\n",
      "[epoch 8] val_loss: 0.428  val_accuracy: 0.902\n",
      "Training_Time: 114.23 seconds\n",
      "train epoch[9/25] loss:0.063: 100%|██████████| 282/282 [01:36<00:00,  2.92it/s]\n",
      "valid epoch[9/25]: 100%|██████████| 55/55 [00:21<00:00,  2.53it/s]\n",
      "[epoch 9] train_loss: 0.032  train_accuracy: 98.824\n",
      "[epoch 9] val_loss: 0.417  val_accuracy: 0.895\n",
      "Training_Time: 118.29 seconds\n",
      "train epoch[10/25] loss:0.053: 100%|██████████| 282/282 [01:32<00:00,  3.05it/s]\n",
      "valid epoch[10/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 10] train_loss: 0.028  train_accuracy: 98.992\n",
      "[epoch 10] val_loss: 0.394  val_accuracy: 0.900\n",
      "Training_Time: 114.00 seconds\n",
      "train epoch[11/25] loss:0.002: 100%|██████████| 282/282 [01:33<00:00,  3.01it/s]\n",
      "valid epoch[11/25]: 100%|██████████| 55/55 [00:21<00:00,  2.53it/s]\n",
      "[epoch 11] train_loss: 0.025  train_accuracy: 99.094\n",
      "[epoch 11] val_loss: 0.462  val_accuracy: 0.900\n",
      "Training_Time: 115.55 seconds\n",
      "train epoch[12/25] loss:0.052: 100%|██████████| 282/282 [01:32<00:00,  3.04it/s]\n",
      "valid epoch[12/25]: 100%|██████████| 55/55 [00:21<00:00,  2.56it/s]\n",
      "[epoch 12] train_loss: 0.026  train_accuracy: 99.061\n",
      "[epoch 12] val_loss: 0.538  val_accuracy: 0.884\n",
      "Training_Time: 114.22 seconds\n",
      "train epoch[13/25] loss:0.030: 100%|██████████| 282/282 [01:33<00:00,  3.03it/s]\n",
      "valid epoch[13/25]: 100%|██████████| 55/55 [00:21<00:00,  2.54it/s]\n",
      "[epoch 13] train_loss: 0.022  train_accuracy: 99.207\n",
      "[epoch 13] val_loss: 0.559  val_accuracy: 0.889\n",
      "Training_Time: 114.80 seconds\n",
      "train epoch[14/25] loss:0.008: 100%|██████████| 282/282 [01:32<00:00,  3.06it/s]\n",
      "valid epoch[14/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 14] train_loss: 0.024  train_accuracy: 99.151\n",
      "[epoch 14] val_loss: 0.493  val_accuracy: 0.904\n",
      "Training_Time: 113.76 seconds\n",
      "train epoch[15/25] loss:0.004: 100%|██████████| 282/282 [01:33<00:00,  3.01it/s]\n",
      "valid epoch[15/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 15] train_loss: 0.020  train_accuracy: 99.317\n",
      "[epoch 15] val_loss: 0.511  val_accuracy: 0.896\n",
      "Training_Time: 115.24 seconds\n",
      "train epoch[16/25] loss:0.005: 100%|██████████| 282/282 [01:32<00:00,  3.05it/s]\n",
      "valid epoch[16/25]: 100%|██████████| 55/55 [00:22<00:00,  2.50it/s]\n",
      "[epoch 16] train_loss: 0.019  train_accuracy: 99.318\n",
      "[epoch 16] val_loss: 0.491  val_accuracy: 0.889\n",
      "Training_Time: 114.55 seconds\n",
      "train epoch[17/25] loss:0.140: 100%|██████████| 282/282 [01:32<00:00,  3.04it/s]\n",
      "valid epoch[17/25]: 100%|██████████| 55/55 [00:21<00:00,  2.56it/s]\n",
      "[epoch 17] train_loss: 0.018  train_accuracy: 99.436\n",
      "[epoch 17] val_loss: 0.416  val_accuracy: 0.908\n",
      "Training_Time: 114.39 seconds\n",
      "train epoch[18/25] loss:0.020: 100%|██████████| 282/282 [01:31<00:00,  3.07it/s]\n",
      "valid epoch[18/25]: 100%|██████████| 55/55 [00:20<00:00,  2.62it/s]\n",
      "[epoch 18] train_loss: 0.023  train_accuracy: 99.224\n",
      "[epoch 18] val_loss: 0.574  val_accuracy: 0.889\n",
      "Training_Time: 112.73 seconds\n",
      "train epoch[19/25] loss:0.007: 100%|██████████| 282/282 [01:32<00:00,  3.05it/s]\n",
      "valid epoch[19/25]: 100%|██████████| 55/55 [00:21<00:00,  2.53it/s]\n",
      "[epoch 19] train_loss: 0.017  train_accuracy: 99.404\n",
      "[epoch 19] val_loss: 0.604  val_accuracy: 0.910\n",
      "Training_Time: 114.46 seconds\n",
      "train epoch[20/25] loss:0.055: 100%|██████████| 282/282 [01:32<00:00,  3.04it/s]\n",
      "valid epoch[20/25]: 100%|██████████| 55/55 [00:22<00:00,  2.46it/s]\n",
      "[epoch 20] train_loss: 0.022  train_accuracy: 99.275\n",
      "[epoch 20] val_loss: 0.555  val_accuracy: 0.891\n",
      "Training_Time: 115.26 seconds\n",
      "train epoch[21/25] loss:0.134: 100%|██████████| 282/282 [01:32<00:00,  3.07it/s]\n",
      "valid epoch[21/25]: 100%|██████████| 55/55 [00:21<00:00,  2.52it/s]\n",
      "[epoch 21] train_loss: 0.020  train_accuracy: 99.279\n",
      "[epoch 21] val_loss: 0.587  val_accuracy: 0.890\n",
      "Training_Time: 113.86 seconds\n",
      "train epoch[22/25] loss:0.034: 100%|██████████| 282/282 [01:32<00:00,  3.06it/s]\n",
      "valid epoch[22/25]: 100%|██████████| 55/55 [00:21<00:00,  2.59it/s]\n",
      "[epoch 22] train_loss: 0.018  train_accuracy: 99.360\n",
      "[epoch 22] val_loss: 0.641  val_accuracy: 0.890\n",
      "Training_Time: 113.27 seconds\n",
      "train epoch[23/25] loss:0.002: 100%|██████████| 282/282 [01:33<00:00,  3.02it/s]\n",
      "valid epoch[23/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 23] train_loss: 0.014  train_accuracy: 99.506\n",
      "[epoch 23] val_loss: 0.680  val_accuracy: 0.893\n",
      "Training_Time: 114.99 seconds\n",
      "train epoch[24/25] loss:0.004: 100%|██████████| 282/282 [01:33<00:00,  3.02it/s]\n",
      "valid epoch[24/25]: 100%|██████████| 55/55 [00:21<00:00,  2.55it/s]\n",
      "[epoch 24] train_loss: 0.013  train_accuracy: 99.506\n",
      "[epoch 24] val_loss: 0.703  val_accuracy: 0.891\n",
      "Training_Time: 114.85 seconds\n",
      "train epoch[25/25] loss:0.004: 100%|██████████| 282/282 [01:33<00:00,  3.01it/s]\n",
      "valid epoch[25/25]: 100%|██████████| 55/55 [00:21<00:00,  2.59it/s]\n",
      "[epoch 25] train_loss: 0.015  train_accuracy: 99.479\n",
      "[epoch 25] val_loss: 0.564  val_accuracy: 0.904\n",
      "Training_Time: 115.03 seconds\n",
      "訓練完成\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "# 訓練和驗證模型\n",
    "num_epochs = 25\n",
    "best_acc = 0.0\n",
    "train_steps = len(train_loader)\n",
    "save_path = 'best_mobilenetv3L.pth'\n",
    "t_l, t_a = [], []\n",
    "v_l, v_a = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_accuracy = train(epoch, num_epochs, model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accurate = validate(epoch, num_epochs, model, val_loader, criterion, device)\n",
    "    t_l.append(train_loss)\n",
    "    t_a.append(train_accuracy)\n",
    "    v_l.append(val_loss)\n",
    "    v_a.append(val_accurate)\n",
    "    \n",
    "    print('[epoch %d] train_loss: %.3f  train_accuracy: %.3f' %\n",
    "            (epoch + 1, train_loss, train_accuracy))\n",
    "    print('[epoch %d] val_loss: %.3f  val_accuracy: %.3f' %\n",
    "            (epoch + 1, val_loss, val_accurate))\n",
    "    \n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    end_time = time.time()\n",
    "    print(f'Training_Time: {end_time - start_time:.2f} seconds')\n",
    "    \n",
    "print('訓練完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7d468a7-f734-406c-bdf0-2a2ac74dd5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.9102\n"
     ]
    }
   ],
   "source": [
    "# 測試模型\n",
    "model.load_state_dict(torch.load('best_mobilenetv3L.pth'))\n",
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_running_corrects.double() / len(test_dataset)\n",
    "print(f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1471b0aa-72cb-4d7b-b990-56c8189ee85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_file(t_l, t_a, v_l, v_a, filename='metrics.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"Train Loss:\\n\")\n",
    "        for item in t_l:\n",
    "            file.write(f\"{item}\\n\")\n",
    "        \n",
    "        file.write(\"Train Accuracy:\\n\")\n",
    "        for item in t_a:\n",
    "            file.write(f\"{item}\\n\")\n",
    "        \n",
    "        file.write(\"Validation Loss:\\n\")\n",
    "        for item in v_l:\n",
    "            file.write(f\"{item}\\n\")\n",
    "        \n",
    "        file.write(\"Validation Accuracy:\\n\")\n",
    "        for item in v_a:\n",
    "            file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f67f37-2db0-49bd-89b6-ceff240781ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假設 t_l, t_a, v_l, v_a 已經被填充\n",
    "filename = 'mobilenet.txt'\n",
    "save_metrics_to_file(t_l, t_a, v_l, v_a, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0361e-a196-4568-bfe0-f29cdbc996cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_metrics_from_file(filename='metrics.txt'):\n",
    "    t_l = []\n",
    "    t_a = []\n",
    "    v_l = []\n",
    "    v_a = []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        current_list = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line == \"Train Loss:\":\n",
    "                current_list = t_l\n",
    "            elif line == \"Train Accuracy:\":\n",
    "                current_list = t_a\n",
    "            elif line == \"Validation Loss:\":\n",
    "                current_list = v_l\n",
    "            elif line == \"Validation Accuracy:\":\n",
    "                current_list = v_a\n",
    "            elif line:\n",
    "                current_list.append(float(line))\n",
    "    \n",
    "    return t_l, t_a, v_l, v_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "864ca3dc-33ab-4600-ba64-317f38e6159a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00017941387938076838, 0.00014915590258573905, 0.00014897276882806586, 0.0001344199652537807, 0.0001290054065531068, 0.000123746063941831, 0.00011129486910067498, 0.00011019395043452581, 0.00011555799777852371, 9.799436344635776e-05, 9.115422739543849e-05, 9.921994823445049e-05, 0.00010177693369526727, 9.762458350612886e-05, 9.300677486074467e-05, 9.940537768933508e-05, 8.895165244110911e-05, 8.21552939984637e-05, 8.333765621905008e-05, 7.762585919731969e-05, 7.349414898797274e-05, 8.849507575117362e-05, 8.3043140035847e-05, 7.296231633558313e-05, 7.766158837249451e-05]\n",
      "[98.27083333333333, 98.58333333333333, 98.5875, 98.70138888888889, 98.76944444444445, 98.8236111111111, 98.96527777777777, 99.00972222222222, 98.91805555555555, 99.13055555555556, 99.16805555555555, 99.09027777777777, 99.05972222222222, 99.12638888888888, 99.19027777777778, 99.06527777777778, 99.18055555555556, 99.22777777777777, 99.2625, 99.33194444444445, 99.31111111111112, 99.18333333333334, 99.22222222222223, 99.33055555555555, 99.31111111111112]\n",
      "[0.7060745198045458, 0.6446123119081769, 0.566216237272535, 0.5956349222149168, 0.8275703804492951, 0.5782740439687456, 0.8549351164272854, 0.5367006817204611, 0.5721019596798079, 0.794704601513488, 0.8620648673517364, 0.6634313086100987, 0.6767192225456238, 0.6811768262045724, 0.6794268044744219, 0.9466515711035047, 0.9040828906127385, 0.934220027753285, 0.8434649161781583, 0.8307107735361372, 0.8007975097724369, 0.8674803879431315, 1.1184742212976728, 1.290574423931539, 0.7036815576212747]\n",
      "[0.8575714285714285, 0.8627142857142858, 0.8545714285714285, 0.8547857142857143, 0.8335714285714285, 0.8610714285714286, 0.8430714285714286, 0.8659285714285714, 0.8652857142857143, 0.8655, 0.8607142857142858, 0.8622857142857143, 0.861, 0.8600714285714286, 0.8538571428571429, 0.8198571428571428, 0.8327142857142857, 0.8258571428571428, 0.8414285714285714, 0.8571428571428571, 0.8497857142857143, 0.8337857142857142, 0.8254285714285714, 0.8169285714285714, 0.8556428571428571]\n"
     ]
    }
   ],
   "source": [
    "# 從文件中讀取\n",
    "t_l, t_a, v_l, v_a = load_metrics_from_file(filename)\n",
    "\n",
    "# 確認讀取的數據\n",
    "print(t_l)\n",
    "print(t_a)\n",
    "print(v_l)\n",
    "print(v_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2344b1e4-a396-4a99-8eba-b18abcb3e419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from mobilenetv3 import mobilenetv3_large\n",
    "\n",
    "# 加載預訓練的 MobileNetV3 大模型\n",
    "model = mobilenetv3_large(num_classes=2)\n",
    "model.load_state_dict(torch.load('best_mobilenetv3L.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 使用 TorchScript 將模型保存為 .pt 文件\n",
    "example_input = torch.rand(1, 3, 224, 224)\n",
    "traced_script_module = torch.jit.trace(model, example_input)\n",
    "torch.jit.save(traced_script_module, 'mobilenet_v3_jit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1640cbae-88b7-47da-923e-0fb4b647fbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_celeb_dataset = datasets.ImageFolder(root='../CelebDF_v2/extracted_frames', transform=transform)\n",
    "celeb_loader = DataLoader(test_celeb_dataset, batch_size=256, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ca3e21-b7a2-4f26-bb61-6b3fbfcfee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc of Celeb DF V2: 0.7388\n"
     ]
    }
   ],
   "source": [
    "# 測試模型\n",
    "model.load_state_dict(torch.load('best_mobilenetv3L.pth'))\n",
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in celeb_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_running_corrects.double() / len(test_dataset)\n",
    "print(f'Test Acc of Celeb DF V2: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95596830-d4b2-4e44-9737-1d98d6d6dc39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
